import ngramUtilities
import pandas as pd
import generateCompleteProteome
import networkx as nx
import warnings
import random
import numpy as np
import itertools
import matplotlib.pyplot as plt
from tqdm import tqdm
import scipy.stats as stats

class dansy:
    '''
    A domain n-gram network built off either a list of proteins of interest or a reference file generated by CoDIAC. If InterPro IDs are provided will extract n-grams that contain only those IDs. Default values will generate a 10-gram network model.

    Parameters:
    -----------
        - protsOI: list
            - List of UniProt IDs whose n-grams are desired to generate the n-gram network.
        - ref: pandas DataFrame (Recommended)
            - Dataframe that has been generated from CoDIAC containing both InterPro and UniProt information.
        - n: int (Optional)
            - N-gram lengths to be extracted
        - interproIDs: list (Optional)
            - List of Interpro IDs to extract n-grams. If omitted, all n-grams will be extracted.

    '''
    def __init__(self,protsOI = None, ref = None,n = 10,interproIDs = None, **kwargs):
        
        # Bare minimum attributes required for setting up an empty n-gram network.
        self.protsOI = protsOI
        self.ref = ref
        self._n = n
        self.interproIDs = interproIDs

        # If any parameters were included that can generate the n-gram network actually populating and generating the network.
        if (self.protsOI is not None) or (self.ref is not None):
            self.populate_ngramNet(**kwargs)
        else:
            self.network_params = {}
    
    @property
    def n(self):
        return self._n
    
    @n.setter
    def n(self, value):
        raise AttributeError('The maximum length of n-gram (n) cannot be readjusted.')

    def populate_ngramNet(self,**attribs):
        '''
        This will actually generate the n-gram network object and go through the domain n-gram analysis and populate each attribute with the user defined and default parameters.
        '''
        # Getting the Proteins of interest that will be used for the analysis. If proteins of interest are not provided then will raise an error.
        self.add_prots()
        self.add_ref_df()
        
        # Unpack the additional, optional parameters that alter the behavior of the network generated or provide descriptions.
        self.add_interpro_ids()
        self.set_ngram_parameters(**attribs)
        self.unpack_opts(**attribs)

        # Now generating the actual Domain N-gram Network that is the basis of this class
        self.build_dgNet()
    
    # Adding in protein of interest
    def add_prots(self):
        '''
        Adds in the protein list if it was not provided
        '''

        # If the protein list is not provided but the reference file is, then import the entire protein list from the reference file
        if self.protsOI is None and self.ref is not None:
            ref = self.check_ref(self.ref)
            self.protsOI = self.extract_uniprots(ref)
        
        # Ensuring the correct data type for downstream analysis
        if isinstance(self.protsOI, str):
            self.protsOI = [self.protsOI]
        elif isinstance(self.protsOI, list):
            self.protsOI = self.protsOI
        elif isinstance(self.protsOI, set):
            self.protsOI = list(self.protsOI)
        else:
            raise ValueError('UniProt IDs should be provided as a list.')

    # Adding in the reference dataframe for the n-gram analysis
    def add_ref_df(self):
        '''
        Adds in the dataframe that contains all the reference information for the protein domain architectures.
        '''
        # if no reference file is provided then this will load the current whole proteome reference file in the designated directory.
        
        # Import the complete proteome reference file if no reference file has been specified.
        if self.ref is None:
            ref_data,_ = generateCompleteProteome.import_proteome_files()
        
        # Ensure the reference data is a dataframe and not just the string to the reference file.
        else:
            ref_data = self.check_ref(self.ref)
        self.get_refs(ref_data)

    def add_interpro_ids(self):
        '''
        Add the list of interpro ids that will be used to find the n-grams of interest.
        '''
        # Setting up the InterPro IDs that will be used to extract the n-grams
        if self.interproIDs is None:
            tokens = ngramUtilities.generate_interpro_conversion(self.ref)
            self.interproIDs = list(tokens.keys())


    # Getting the reference data frame used for each instance of the class
    def get_refs(self,ref_df):
        self.ref = ref_df[ref_df['UniProt ID'].isin(self.protsOI)]

    # Building the actual domain n-gram network
    def build_dgNet(self):
        self.adj,_,self.interpro2uniprot,self.collapsed_ngrams,self.interpro_conversion = ngramUtilities.full_ngram_analysis(self.ref,
                                                                                                                            self.interproIDs,
                                                                                                                            max_ngram=self.n, 
                                                                                                                            min_arch=self.min_arch,
                                                                                                                            readable_flag=self.readable_flag,
                                                                                                                            max_node_len=self.max_node_len,
                                                                                                                            concat_flag=self.collapse,
                                                                                                                            verbose=self.verbose)

        # Get the full list of n-grams
        self.ngrams = list(self.adj.columns)
        self.generate_network()

    def generate_network(self):
        '''
        Uses networkx to generate a network view of the n-gram network. This will remove the self-loop edges from the n-gram network, but does not touch the adjacency matrix.
        '''
        # Using networkx to generate the network and then remove the self loops.
        
        # In some instances all provided proteins will have the exact same domain architecture so a networkx graph generation is somewhat useless and fails 
        if len(self.adj.columns) > 1:
            G = nx.from_pandas_adjacency(self.adj)
            G.remove_edges_from(nx.selfloop_edges(G))
            self.G = G
        else:
            if self.verbose:
                print('Only one n-gram was found in the dataset. Please double-check inputs.')
            G = nx.Graph()
            G.add_node(self.adj.columns[0])
            self.G = G

    
    # Utility to draw basic network, but is recommended to use the actual networkx package.
    def draw_network(self):
        '''
        Draws a basic version of the Graph with the networkx spring layout implementation. It is recommended to use the actual networkx package for a full implementation.
        '''
        
        # Default values 
        basic_network_params = {'node_size':1,
                                  'edgecolors':'k',
                                  'linewidths':0.1,
                                  'width':0.25,
                                  'edge_color':'#808080',
                                  }
        
        # Retrieving any of the network values that were set as attributes and placing into the network parameters
        if self.network_params:
            for k in basic_network_params.keys():
                if k in self.network_params:
                    basic_network_params[k] = self.network_params[k]

        if 'pos' in self.network_params:
            basic_network_params['pos'] = self.network_params['pos']

        nx.draw(self.G,**basic_network_params)


    # Check the reference file status
    @staticmethod
    def check_ref(ref):
        '''
        Checks whether the provided reference is a string to a specific file or an already imported DataFrame. It will return the actual DataFrame that was imported.
        '''
        if isinstance(ref, str):
            ref_df = ngramUtilities.import_reference_file(ref)
        elif isinstance(ref, pd.DataFrame):
            ref_df = ref
        else:
            raise TypeError('Reference File is not correct')
        
        return ref_df
    
    # Extract all UniProt IDs from a reference file if only the reference file is to be used
    @staticmethod
    def extract_uniprots(ref):
        '''
        Extracts a list of the entire UniProts that were contained within a provided reference file.
        '''

        if 'UniProt ID' in ref.columns:
            uniprot_ids = ref['UniProt ID'].tolist()
        else:
            raise ValueError('The reference file is missing a UniProt ID column. Please check or regenerate the reference file.')
        
        return uniprot_ids

    # Return a human readable version of specified n-grams
    def return_legible_ngram(self, ngram):
        '''
        Given an n-gram of interest. It will convert the name to a human legible version.
        '''

        if '|' in ngram:
            split_ngram = ngram.split('|')
            ngram_conv = []
            for dom in split_ngram:
                ngram_conv.append(self.interpro_conversion[dom])
            ngram_conv = '|'.join(ngram_conv)
        else:
            ngram_conv = self.interpro_conversion[ngram]
        
        return ngram_conv
    
    def ngram_protein_count(self, ngram):
        '''
        Returns the number of proteins an individual n-gram was found within.
        '''
        if ngram in self.collapsed_ngrams:
            cands = [i for i in self.ngrams if ngram in i]
            if len(cands) > 1:
                raise ValueError('The n-gram of interest was somehow found across several non-redundant n-grams?')
            else:
                c = len(self.interpro2uniprot[cands[0]])
        elif ngram in self.ngrams:
            c = len(self.interpro2uniprot[ngram])
        else:
            raise ValueError('The inputted n-gram is not found.')

        return c

    def unpack_opts(self, **kwargs):
        '''
        Unpacks both necessary and optional attributes that can be used for downstream analysis. It will check for a list of acceptable optional parameters that can be set with some being necessary for the analysis and others to help with record-keeping/descriptions.
        '''
        
        opts = ['name',
                'collapse',
                'max_node_len',
                'min_arch',
                'readable_flag',
                'verbose']

        network_opts = ['pos',
                        'node_size',
                        'node_color',
                        'linewidths',
                        'edge_color',
                        'width',
                        'edgecolors']
        ignored_opts = []
        
        for k,v in kwargs.items():
            if k in opts:
                self.__setattr__(k,v)
            elif k in network_opts:
                pass # separate function to unpack the network associated parameters
            else:
                ignored_opts.append(k)
        
        network_params = self.unpack_net_opts(**kwargs)
        
        self.network_params = network_params

        if ignored_opts and self.verbose:
            print(f'The following parameters were ignored:{ignored_opts}')

    def unpack_net_opts(self, **kwargs):
        '''
        Unpacks key value parameters that relate to the basic drawing of a network. This does not fully encompass all parameters found within an networkx function.
        '''

        network_opts = ['pos',
                        'node_size',
                        'node_color',
                        'linewidths',
                        'edge_color',
                        'width',
                        'edgecolors']

        network_params = {}
        for k,v in kwargs.items():
            if k in network_opts:
                network_params[k] = v

        return network_params


    def set_ngram_parameters(self, **kwargs):
        '''
        Using the parameters given override defaults as necessary.
        '''

        # Default values for the n-gram analysis
        ngram_parameters = {'min_arch':1,
                          'max_node_len':None,
                          'collapse':True,
                          'readable_flag':False,
                          'verbose':True}
        
        for k,v in kwargs.items():
            if k in ngram_parameters.keys():
                ngram_parameters[k] = v

        for k,v in ngram_parameters.items():
            self.__setattr__(k, v)

    def summary(self, detailed = False):
        '''
        Output a summary of key features that are represented within the n-gram network.
        '''

        # Generating the baseline dataframe that will hold the summarized data.

        index = ['name',
                 'Proteins',
                 'n-grams',
                 'Network Isolates',
                 'Network Connected Components',
                 ]
        
        net_sum = pd.DataFrame(index=index,columns=[''])
        if 'name' in self.__dict__:
            net_sum.loc['name'] = self.name
        else:
            net_sum.loc['name'] = 'Domain n-gram Network'
        net_sum.loc['Proteins'] = len(self.protsOI)
        net_sum.loc['n-grams'] = len(self.ngrams)
        net_sum.loc['Network Isolates'] = nx.number_of_isolates(self.G)
        net_sum.loc['Network Connected Components'] = nx.number_connected_components(self.G)

        if detailed:
            net_sum.loc['Collapsed n-grams'] = len(self.collapsed_ngrams)
            net_sum.loc['Network Edges'] = nx.number_of_edges(self.G)
            net_sum.loc['Maximum Length of Protein Domain Architecture'] = self.retrieve_longest_arch(self.ref)


        return net_sum
    
    @staticmethod
    def retrieve_longest_arch(ref):
        '''Retrieves the longest domain architecture length.'''

        arch_max = max(ref['Interpro Domain Architecture IDs'].apply(lambda x: len(x.split('|'))))

        return arch_max

    def import_precomp_data(self,adj,ref,interpro_ids,n,collapsed_ngrams = None,**net_attribs):
        '''
        For instances where a precomputed n-gram network was generated already, this will import the data if it does not already exist. 
        '''

        # Check pre-existence of any values
        if ('adj' in self.__dict__) or (self.ref is not None) or (self.interproIDs is not None):
            raise ValueError('Cannot alter already computed n-gram network parameters. Please create a new instance.')
        
        else:
            self.adj = adj
            self.ref = self.check_ref(ref)
            self.interproIDs = interpro_ids
            self._n = n

        # Populate the n-gram lists
        self.ngrams = list(self.adj.columns)
        if collapsed_ngrams is not None:
            self.collapsed_ngrams = collapsed_ngrams
        
        self.validate_n()

        # Create the n-gram to UniProt mapping. This uses the preexisting function.
        _,ngram_dict = ngramUtilities.get_ngrams_from_df(self.ref, self.interproIDs,arch_num=1, max_ngram=self.n)

        # For the n-grams that were already collapsed removing from the n-gram dict to reduce redundant information.
        if collapsed_ngrams is None:
            self.interpro2uniprot = ngram_dict
        else:
            for ngram in collapsed_ngrams:
                del ngram_dict[ngram]
            self.interpro2uniprot = ngram_dict

        # Unpacking only network options
        self.network_params = self.unpack_net_opts(**net_attribs)

        self.generate_network()


    def validate_n(self):
        '''
        Ensure that the n that has been provided matches the n-gram model that has been provided.
        '''

        # Check n-grams within the adjacency matrix
        cur_max_n = max([len(x.split('|')) for x in self.ngrams])
        max_possible = self.retrieve_longest_arch(self.ref)
        
        # First checks that there is no mismatch in longest extracted n-gram.
        if cur_max_n > self.n:
            raise AttributeError('The provided max n-gram length is longer than n-grams in the provided n-gram list.')
        
        if 'collapsed_ngrams' in self.__dict__:
            col_max = max([len(x.split('|')) for x in self.collapsed_ngrams])
            if col_max > self.n:
                raise AttributeError('The provided max n-gram length is longer than n-grams in the provided n-gram list.')
            
        # Now checking whether there may have just been a mistake of the longest and raise a warning. If ther maximum length is much smaller than the maximum possible length and that does not match the provided n
        if (cur_max_n < max_possible) and (self.n != cur_max_n):
            raise AttributeError('The provided max n-gram does not match the provided n-gram list.')
        if self.n >  max_possible:
            warnings.warn('The maximum possible n-gram that can be extracted was smaller than the desired maximum n-gram length.')

    def retrieve_protein_info(self, prot = None, ngram = None):
        '''
        Retrieves the reference information of proteins of interest. If an InterPro ID/n-gram is provided it will use that instead and search for the proteins with that ID and return that instead.
        '''

        if prot == None and ngram == None:
            raise TypeError('Either a protein list or an n-gram are necessary. Please provide either one.')
        elif prot != None and ngram != None:
            raise TypeError('Please provide either a protein list or an n-gram and not both.')
        
        if ngram != None:
            prot = self.interpro2uniprot[ngram]
        
        if isinstance(prot, str):
            prot = [prot]

        prot_info = self.ref[self.ref['UniProt ID'].isin(prot)]

        return prot_info
    
    def retrieve_random_ids(self,num, iters = 50, seed = 882):
        '''Generator of random UniProt IDs from the base network.'''
        random.seed(seed)
        seedlist = random.sample(range(100000), k=iters)
        full_id_list = sorted(self.ref['UniProt ID'].tolist())
        for i in range(iters):
            random.seed(seedlist[i])
            rand_ids = random.sample(full_id_list, k = num)
            yield rand_ids

def mean_minimum_dist(G_nodes, ref_net_params_dict):
    '''
    Calculates the average shortest path length between nodes within the network along a reference network. Nodes that are isolates within the reference network have a shortest path length of 0, and nodes within the network which represent a completely collapses connected component also have a shortest path length of 0 or a penalty value if provided. 
    '''
    # Unpacking the reference information that is used in calculating the shortest path lengths
    ref_isolates = ref_net_params_dict['isolates']

    if isinstance(G_nodes, nx.Graph):
        nodesOI = set(G_nodes.nodes())
    else:
        nodesOI = G_nodes

    if len(nodesOI) > 0:
        # Don't bother checking the expected isolates. These will be accounted for in the total length as they contribute as 0s in the final calculation.
        nodes_2_check = nodesOI.difference(ref_isolates)
        
        # Get the shortest path length for individual nodes that do exist in the reference network
        spl = spl_inner(nodes_2_check, nodesOI, ref_net_params_dict)
        
        # Ensuring the isolates are accounted for in the average
        mean_min_dist = np.sum(spl)/len(nodesOI)
    
    else:
        mean_min_dist = np.nan

    return mean_min_dist

def spl_inner(nodes_2_check, all_nodes, ref_net_analysis_parameters):
    '''
    An inner calculation function that gets the shortest path length between nodes of interest and target nodes that are often found within 2 subnetworks of a larger network. For nodes that do not have connections between the subnetworks these are given a penalty term that is defined in the reference network information provided.

    Parameters:
    -----------
        - nodes_2_check: list
            - Nodes of interest who act as a source of the shortest path lengths are to be gathered
        - all_nodes: list
            - The target list of nodes that the shortest path distance from the nodes of interest is to be queried for.
        - ref_net_analysis_parameters: dict
            - A dictionary that contains several components of the reference, background network that are used in the calculation. See build_network_reference_dict for more details.

    Returns:
    --------
        - spl: list
            - The shortest path distances for all the nodes of interest. (Note that if there are no connections between a node of interest and the target nodes, this will be the penalty term found in the reference network dict.)
    
    '''
    
    reference_spl_dict = ref_net_analysis_parameters['spl']
    penalty_input = ref_net_analysis_parameters['penalty']

    # Verify that the penalty that was to be used is an appropriate format and can be used for downstream analysis
    dynamic_flag = False
    # If a constant term is provided
    if isinstance(penalty_input, int):
        penalty = penalty_input
    # If one of the acceptable methods is provided and thus being calculated
    elif penalty_input == 'dynamic':
        dynamic_flag = True
    else:
        raise TypeError('Inappropriate penalty term input')

    # Some precomputing for the dynamic method
    if dynamic_flag:
    
        if 'CCs' in ref_net_analysis_parameters and 'cc_diameters' in ref_net_analysis_parameters:
            ccs = ref_net_analysis_parameters['CCs']
            cc_diams = ref_net_analysis_parameters['cc_diameters']
        else:
           raise ValueError('An invalid reference data dictionary was provided. Please run build_network_reference_dict')
        
        # Reduce the connected components that are being referenced are only the ones that contain nodes within the network.
        ccs = [cc for cc in ccs if cc.intersection(nodes_2_check)]
        
        # Fail-safe check for debugging
        if nodes_2_check:
            if (not cc_diams) & (len(ccs) > 0):      
                raise ValueError(f'Whoops something went very wrong and this should not happen.')
    
    # In some instances there may be some nodes removed from the spl dict (via pruning), so will filter out nodes that are not a part of that
    orig_nodes = len(nodes_2_check)
    nodes_2_check = set(nodes_2_check).intersection(reference_spl_dict.keys())

    # If there was a pruned shortest path length dict passed then ensuring that the full complement of nodes does not include the pruned nodes
    if len(nodes_2_check) != orig_nodes:
        all_nodes = set(all_nodes).intersection(reference_spl_dict.keys())


    # Now getting the shortest path lengths based on the provided parameters
    spl = []
    for node in nodes_2_check:
        tmp_spl = reference_spl_dict[node]
        spl_dict = {k:v for k,v in tmp_spl.items() if k in all_nodes}
        if node in spl_dict:
            del spl_dict[node] # Removing the self-reference that is always 0

        if spl_dict:
            spl.append(min(spl_dict.values()))
        else: # If a node by itself represents a single connected component from the reference adding a penalty for collapsing the entire connected component
            
            # The dynamic method 
            if dynamic_flag:
                penalty = cc_diams[node] + 1
                spl.append(penalty)
            else:
                spl.append(penalty)

    return spl


def network_separation(G_in, H_in, ref_G_data, mmd_verbose = False, force_run = False, verbose = True):
    '''
    Calculates the network separation between two networks of interest that lie on a common larger, reference network.

    Parameters:
    -----------
        - G_in, H_in: networkx Graph | DomainNgramNetwork
            - networks of interest
        - ref_G_data: dict
            - dict containing information about the larger, common network that the two given networks are subnetworks of.
        - mmd_verbose: bool (Optional, to be deprecated)
            - Whether a statement is printed.
        - force_run: bool (Optional)
            - Whether the network separation should be run even if it does not meet minimum values necessary.
    
    Returns:
    --------
        - s : float
            - the network separation of two different subnetworks.

    '''
    # Unpacking the reference network information that is used for getting minimum distance values
    ref_isolates = ref_G_data['isolates']

    # Now getting the minimum distances for both the sub-networks of interest
    # As part of the mean minimum distance calculation there is a secondary option to set penalties for disconnected nodes. If it is not provided then will set the penalty to zero
    mmd_ref = {**ref_G_data} # Need to create a copy otherwise can overwrite the penalty value by accident
    if 'mean_dist_penalty' in ref_G_data:
        if isinstance(ref_G_data['mean_dist_penalty'], int): # To allow a separate penalty term for the mean shortest path distance calculation from the network separation
            mmd_ref['penalty'] = ref_G_data['mean_dist_penalty']
        elif isinstance(ref_G_data['mean_dist_penalty'], bool):
            if ref_G_data['mean_dist_penalty']:
                pass # Already has the penalty term passed
            else:
                mmd_ref['penalty'] = 0     
        else:
            raise ValueError('An improper penalty value was provided for calculating the mean minimum distance.')   
    else:
        mmd_ref['penalty'] = 0

    # After some parameter sweeps there is a rough cutoff for networks that represent ~20 proteins that tend to exhibit wide ranges in network separation values that are not as informative to network characteristics as they frequently will have larger isolate fractions than landing within a large connected component. Here checking that both networks meet the minimum size otherwise raising an error. (Unless the force run flag is present.)
    if isinstance(G_in, dansy) and isinstance(H_in, dansy):  
        if (len(G_in.protsOI) < 20) or (len(H_in.protsOI) < 20):
            if force_run:
                if verbose: 
                    warnings.warn('At least one network does not reach recommended minimum size, but will still be analyzed.')
            else:
                raise ValueError('At least one network does not reach recommended minimum size.')
            
        G = G_in.G
        H = H_in.G
    else:
        G = G_in
        H = H_in

    # Getting the list/set of nodes for doing calculations on
    G_v = set(G.nodes())
    H_v = set(H.nodes())

    if mmd_verbose: # This is for debugging purposes but can be retained if needed.
        print(f"Input penalty for the mean minimum distance is {mmd_ref['penalty']}")
    G1_mmd = mean_minimum_dist(G_v,mmd_ref)
    G2_mmd = mean_minimum_dist(H_v,mmd_ref)

    # To find the separation between the two networks will find the minimum distance from nodes in one network to nodes in the other network.
    # First finding the overlap in nodes as these will be 0 and don't need to go through the process.
    node_overlap = G_v.intersection(H_v)
    nodesOI_1 = G_v.difference(node_overlap)
    nodesOI_2 = H_v.difference(node_overlap)

    # For the nodes that were isolates in the reference network removing them from running in the for loop as they will not provide any additional information.
    nodesOI_1 = nodesOI_1.difference(ref_isolates)
    nodesOI_2 = nodesOI_2.difference(ref_isolates)
    
    # Running through the non-isolate nodes and getting the shortest distances from nodes within one network to nodes in another.
    spl_union = spl_inner(nodesOI_1, H_v,ref_G_data)
    tmp = spl_inner(nodesOI_2,G_v,ref_G_data)
    spl_union += tmp

    # Now for the isolates to account for their contributions and creating a constant 1 path length penalty for those that are not shared between networks
    isols_1 = G_v.intersection(ref_isolates)
    isols_2 = H_v.intersection(ref_isolates)
    isol_correction_1 = len(isols_1.difference(isols_2))
    isol_correction_2 = len(isols_2.difference(isols_1))

    # Getting the average minimum distance across the union of both networks
    union_mmd = (sum(spl_union)+isol_correction_1+isol_correction_2)/(G.number_of_nodes()+H.number_of_nodes())
    s = union_mmd - (G1_mmd+G2_mmd)/2

    return s


class DEdansy(dansy):
    '''
    A container class of multiple Domain n-gram networks related to a differentially expressed dataset that was generated using the DESeq analysis pipeline. This provides methods to analyze and contrast pairs of domain architecture subnetworks to understand changes in functional molecular ecosystems available to different conditions.
    '''
    def __init__(self,dataset, id_conv, conv_cols = 'Gene stable ID', data_ids = 'gene_id', uniprot_ref = None,n = 10, penalty = 'dynamic',run_conversion = True, **kwargs):
        
        # Bare minimum attributes required for setting up an empty n-gram network.
        self.dataset = dataset
        self.ref = uniprot_ref
        self._n = n
        self.interproIDs = None
        
        # Converting the dataset ids for individual genes to the UniProt IDs
        if 'dbl_check' in kwargs:
            check_IDs_flag = kwargs['dbl_check']
        else:
            check_IDs_flag = False
        
        if run_conversion:
            self.id_conversion_dict = create_id_conv(dataset, id_conv, conv_cols,data_ids,check_IDs_flag)
            self.protsOI = convert_2_uniprotIDs(dataset,id_conv, conv_cols,data_ids, check_IDs_flag)
        else:
            self.protsOI = dataset[data_ids].tolist()

        # Saving the data_id column for instances when the DEGs have to be calculated.
        if isinstance(data_ids, list):
            self.data_id_col = data_ids[0]
        else:
            self.data_id_col = data_ids

        self.conversion_id_col = conv_cols

        # Now making sure there is a common reference that can be used for generating the n-gram networks.
        if self.ref is None:
            self.add_ref_df()
        
        self.populate_ngramNet(**kwargs)
        if self.verbose:
            print('Building the reference network information.')
        self.ref_data = build_network_reference_dict(self, penalty=penalty)


    def DEG_network_sep(self, force_run = False):
        ''' 
        This computes the network separation of two conditions that designates individual genes as differentially expressed. Silently passes a nan if it fails.
        '''

        # Get the UniProt IDs for the differentially expressed genes
        if hasattr(self, 'up_DEGs') and hasattr(self, 'down_DEGs'):
            pass
        else:
            raise ValueError('DEGs do not exist. Please run calc_DEG_ngrams.')
        
        if len(self.up_ngrams) == 0 or len(self.down_ngrams) == 0:
            ns = np.nan
        else:
            try:
                # Generating networks for the individual DEG conditions
                up_net = self.G.subgraph(self.up_ngrams)
                dn_net = self.G.subgraph(self.down_ngrams)
                ns = network_separation(up_net,dn_net, self.ref_data,force_run=force_run)
            except:
                ns = np.nan
        return ns
    
    def calc_DEG_ngrams(self, data_cols, alpha = 0.05, fc_thres=1, contrast_name = None, batch_mode = False):
        '''
        Defines the DEG UniProt IDs and the associated n-grams for the datasset of interest
        '''
        
        # If a contrast name has been provided setting that up within the dataset collection
        if contrast_name == None:
            self.DEG_comparison = 'undefined'
        else:
            self.DEG_comparison = contrast_name

        # Checking if there was a set of DEGs that already exists and printing statement saying it will be overwritten.
        if hasattr(self,'up_DEGs'):
            verbose_flag = True
            if hasattr(self, 'G_collapsed'):
                del self.G_collapsed
        else:
            verbose_flag = False

        # This is only for running several and wanting to keep track of general progress.
        if batch_mode:
            verbose_flag = False

        # Recording the thresholds for DEG values
        self.alpha = alpha
        self.fcthres = fc_thres

        # Reducing large dataframe to what is the info needed for generating the DEG networks
        cols = [self.data_id_col]+data_cols
        dataset_OI = self.dataset.filter(cols)

        # Getting the differentially expressed genes
        deg_up = list(dataset_OI[(dataset_OI[data_cols[1]] <= alpha) & (dataset_OI[data_cols[0]] > fc_thres)][self.data_id_col])
        deg_dn = list(dataset_OI[(dataset_OI[data_cols[1]] <= alpha) & (dataset_OI[data_cols[0]] < -fc_thres)][self.data_id_col])

        # Keeping record of the data columns for a summary
        self.pval_data_col = data_cols[1]
        self.fc_data_col = data_cols[0]

        # And converting to the UniProt IDs to highlight n-grams within the network that were retained for each
        up_DEGs = [v for k, v in self.id_conversion_dict.items() if k in deg_up]
        up_DEGs = set(list(itertools.chain.from_iterable(up_DEGs)))
        down_DEGs =  [v for k, v in self.id_conversion_dict.items() if k in deg_dn]
        down_DEGs = set(list(itertools.chain.from_iterable(down_DEGs)))

        # Now just due to random sampling issues making sure the DEGs are in alphabetical order and converting to a list
        up_DEGs = sorted(up_DEGs)
        down_DEGs = sorted(down_DEGs)

        self.set_DEG_ngrams(up_DEGs,down_DEGs, verbose=verbose_flag)

    def set_DEG_ngrams(self, up_DEGs, down_DEGs,collapse = True, verbose=True):
        '''
        This actually sets the DEGs so that they are calculated, but allows for custom sets to be generated for very specific purposes. It is not recommended to use this
        '''
        if hasattr(self,'up_DEGs') and verbose:
            print('Will be overwriting existing DEG data.')

        # Setting the DEGs
        self.up_DEGs = up_DEGs
        self.down_DEGs = down_DEGs

        # Now getting the n-grams associated with the collective DEGs 
        up_ngram_cands = [k for k,v in self.interpro2uniprot.items() if set(v).intersection(self.up_DEGs)]
        down_ngram_cands = [k for k,v in self.interpro2uniprot.items() if set(v).intersection(self.down_DEGs)]

        # Getting the n-gram candidates
        up_ngram_dict = {k:set(v).intersection(self.up_DEGs) for k,v in self.interpro2uniprot.items() if k in up_ngram_cands}
        down_ngram_dict = {k:set(v).intersection(self.down_DEGs) for k,v in self.interpro2uniprot.items() if k in down_ngram_cands}
        
        # Now collapsing these to the non-redundant ones
        if collapse:
            up_ngram_dict,_ = ngramUtilities.concatenate_ngrams(up_ngram_dict)
            down_ngram_dict,_ = ngramUtilities.concatenate_ngrams(down_ngram_dict)
            

        # Exporting to the object
        self.up_ngrams = [k for k in up_ngram_dict.keys()]
        self.down_ngrams = [k for k in down_ngram_dict.keys()]


    def plot_DEG_ns(self,pos = [],deg_labels=[], large_cc_mode=False):
        '''
        Using the defined differentially expressed genes displaying a network graph of the n-grams that are associated or shared between the DEG conditions.
        '''

        # Setting default labels otherwise setting the label names for the legend.
        if deg_labels:
            up_label = deg_labels[0]
            down_label = deg_labels[1]
        else:
            up_label = 'Up'
            down_label = 'Down'

        # Retrieving the n-grams across both DEG conditions to filter out any isolates/connected components in the reference network that are not used.
        all_deg_ngrams = set(self.up_ngrams).union(self.down_ngrams)

        if large_cc_mode:
            large_ref_cc = max(nx.connected_components(self.G), key=len)
            all_deg_ngrams = all_deg_ngrams.intersection(large_ref_cc)
        
        # Setting up the node color list for plotting
        node_colors = []
        for node in self.G.subgraph(all_deg_ngrams):
            if (node in self.up_ngrams) and (node in self.down_ngrams):
                node_colors.append('tab:purple')
            elif node in self.up_ngrams:
                node_colors.append('tab:cyan')
            elif node in self.down_ngrams:
                node_colors.append('tab:red')
            else:
                node_colors.append('tab:gray')
        
        # Dropping connected components not shared with the DEGs and the reference network
        if large_cc_mode:
            cc_2_keep = set(large_ref_cc)
        else:
            cc_2_keep = set()
            for cc in nx.connected_components(self.G):
                if set(cc).intersection(all_deg_ngrams):
                    cc_2_keep.update(cc)

        # Getting the node positions for network drawing if not supplied.
        if pos == [] and hasattr(self, 'network_params'):
            if 'pos' in self.network_params:
                pos = self.network_params['pos']
        elif pos ==[]:
            pos = nx.spring_layout(self.G, k=0.05)
        
        # Getting all the basic network parameters that are default in the n-gram networks
        # Default values 
        basic_network_params = {'node_size':1,
                                  'edgecolors':'k',
                                  'linewidths':0.1,
                                  'width':0.25,
                                  'edge_color':'#808080',
                                  }
        net_draw_params = {}
        for param in basic_network_params:
            if param in self.network_params:
                net_draw_params[param] = self.network_params[param]
            else:
                self.network_params[param] = basic_network_params[param]
                net_draw_params[param] = basic_network_params[param]

        # Now drawing and adding legends
        plt.figure(figsize=(2,2), dpi=300)
        nx.draw(self.G.subgraph(cc_2_keep), pos, node_color = 'tab:gray', alpha = 0.1, **net_draw_params)
        nx.draw(self.G.subgraph(all_deg_ngrams), pos, node_color = node_colors,**net_draw_params)

        # Legend drawing
        plt.scatter([],[],c='tab:cyan',s=1, label=up_label)
        plt.scatter([],[],c='tab:red',s=1, label=down_label)
        plt.scatter([],[],c='tab:purple',s=1, label='Both')
        plt.legend(bbox_to_anchor=(1,0.5),frameon=False)


    def deg_summary(self, detailed = False):
        '''
        This provides a summary of the DEG information that has been used within this dataset.
        '''

        summary_df = pd.DataFrame(index=['Contrast Name', 'p-value threshold', 'Fold change threshold', 'Up Regulated DEGs','Down Regulated DEGs'],columns=[''])

        vals = [self.DEG_comparison, self.alpha, self.fcthres, len(self.up_DEGs), len(self.down_DEGs)]

        for i,v in zip(summary_df.index, vals):
            summary_df.loc[i] = v
        
        if detailed:
            extended_inds = ['Data ID column','p-val data column', 'FC data column','Up-ngrams','Down ngrams','Common n-grams']

            # Getting common ngram counts
            common = set(self.up_ngrams).intersection(self.down_ngrams)

            extended_vals = [self.data_id_col, self.pval_data_col,self.fc_data_col, len(self.up_ngrams), len(self.down_ngrams), len(common)]

            for i,v in zip(extended_inds, extended_vals):
                summary_df.loc[i] = v

        return summary_df
    
    def ngram_DEG_hypergeom(self, condition):
        '''
        Performs the hypergeometric over-representation test on the n-grams associated with different conditions.

        Parameters:
        -----------
            - condition: str
                Which condition to perform the test on must be either Up or Down (case insensitive)

        Returns:
        --------
            - p_vals: dict
                The p-value of each n-gram given the condition relative to the whole potential universe of the n-gram reference background of the n-gram network.
        '''

        # Make the condition input all lower case
        condition = condition.lower()

        if condition == 'up':
            ngramsOI = self.up_ngrams
            degsOI = self.up_DEGs
        elif condition == 'down':
            ngramsOI = self.down_ngrams
            degsOI = self.down_DEGs
        else:
            raise ValueError('Please specify either up or down.')
        
        p = ngram_enrichment(self, prots = degsOI, ngrams=ngramsOI)

        return p
    
def ngram_enrichment(dansy, prots, collapse = True, **kwargs):
    '''
    Calculates the enrichment of n-grams associated with a subset of proteins within a DANSy object by Fisher's exact test.
    
    Note: This method can specifically analyze specific n-grams within the protein subset. However, this process should only be accessed with the deDANSy object and is not recommended to be used.

    Parameters
    ----------
        - dansy: DANSY
            The DANSy object which contains the proteins of interest. (This can be either the differential expression or standard class.)
        - prots: list
            List containing the proteins of interest for enrichment analysis.
        - collapse: bool
            Whether the n-grams should be collapsed to their most informative and non-redundant n-grams.
        - kwargs: key, value mappings (Not recommended)
            Additional keyword arguments:
                - ngrams: list
                    List of n-grams that are to be analyzed specifically.
    '''

    p_vals = ngram_subset_enrichment(prots, dansy.protsOI, dansy, collapse=collapse, kwargs=kwargs)

    return p_vals

def ngram_subset_enrichment(protsOI, full_prots,dansy_bkg, collapse = True, **kwargs):
    '''Peform Fisher's exact test for n-grams found in a subset of proteins that may be found in the full list of proteins. This is the more general version that does not require using the full DANSy protein list.
    
        Parameters:
        -----------
            - protsOI: list
                List containing the proteins of interest for enrichment analysis.
            - full_prots: list
                List containing the proteins that make up the full background list.
            - dansy_bkg: DANSy object
                The DANSy that the proteins and n-grams are associated with
            - collapse: bool
                Whether the n-grams should be collapsed to their most informative and non-redundant n-grams.
            - kwargs: key, value mappings (Not recommended)
                Additional keyword arguments:
                    - ngrams: list
                        List of n-grams that are to be analyzed specifically.
            
        Returns:
        --------
            - p_vals: dict
                Key-value pairs of n-grams and their enrichment p-value.
    '''

    if 'ngrams' in kwargs:
        ngrams = kwargs['ngrams']
        
        # Making sure all n-grams are found in at least one of the proteins. If any are not raise an error.
        internal_check = [len(set(v).intersection(protsOI)) == 0 for k,v in dansy_bkg.interpro2uniprot.items() if k in ngrams]
        if any(internal_check):
            raise ValueError('At least one provided n-gram is not found in the proteins of interest.')
    else:
        ngrams = []

    subset_prots = list(set(protsOI).intersection(full_prots)) # Ensure that the foreground proteins are found within the background (removing any which are not) 
    M = len(subset_prots)
    N = len(full_prots)
    p_vals = {}
    
    if full_prots == dansy_bkg.protsOI:
        full_check = True
    else:
        full_check = False

    # Get the n-grams of the proteins of interest if they were not provided
    if ngrams:
        ngramsOI = ngrams
    else:
        ngram_cands = [k for k,v in dansy_bkg.interpro2uniprot.items() if set(v).intersection(subset_prots)]
        ngram_dict = {k:set(v).intersection(subset_prots) for k,v in dansy_bkg.interpro2uniprot.items() if k in ngram_cands}

        if collapse:
            ngram_dict,_ = ngramUtilities.concatenate_ngrams(ngram_dict)
        ngramsOI = [k for k in ngram_dict.keys()]
    
    # Now using the cdf/sf of the hypergeometric distribution to get p-values (This is equivalent to Fisher's exact)
    for node in ngramsOI:
        
        # Skip the filtering if using the full background
        if full_check:
            full_prot_list = dansy_bkg.interpro2uniprot[node]
        else:
            
            full_prot_list = [u for u in dansy_bkg.interpro2uniprot[node] if u in full_prots]
            
        # Now getting the numbers for the hypergeom distribution and calculating the cdf (or really sf since it is equivalent to 1-cdf)
        k = len(set(full_prot_list).intersection(subset_prots))
        n = len(set(full_prot_list))
        p = stats.hypergeom.sf(k-1,N, n,M)
        p_vals[node] = p

    return p_vals
    
    

def build_network_reference_dict(ref_ngram_net, penalty = None):
    '''
    This builds the dict that contains the reference network information necessary for calculating the network separation value from a provided DomainNgramNetwork object. 
    
    It is recommended to use this function to generate the reference dictionary prior to calculating network separation, especially when using a dynamic penalty and comparing several networks, to improve the execution speed of the calculation.

    Parameters:
    -----------
        dnn: DomainNgramNetwork
            - A populated Domain N-gram network that will be used as a reference
        penalty: str or int (Optional)
            - What type of penalty will be used for network separation. If not specified will default to the dynamic method.

    Returns:
    --------
        ref_data_dict: dict
            - Key-value pairs that are used for network separation calculations.


    Key-Value Pairs:
        - penalty: int | 'dynamic'
            The penalty term that will be used for shortest path distances
        - spl: dict
            The shortest path length of all pairs of nodes within the network.
        - isolates: list
            List of isolates in the network
        - CCs: list
            List of tuples of nodes that are the connected components of the network
        - cc_diameters: dict
            Key-value pairs of nodes and the diameter of the connected componenet they are a part of.
        - G: networkx Graph
            The graph of the full network.

    '''

    # Default value for the penalty term
    if penalty == None:
        penalty = 'dynamic'

    # Shortest path lengths between individual nodes and non-isolate nodes
    comp_pw_shortest_path_lens = dict(nx.all_pairs_shortest_path_length(ref_ngram_net.G))
    ccs = nx.connected_components(ref_ngram_net.G)
    ccs = [cc for cc in ccs if len(cc) > 1]
    
    # Building a diameter dictionary where the keys are each node and the value is the diameter for the connected component including that node
    cc_diams = {}
    for cc in ccs:
        d = nx.diameter(ref_ngram_net.G.subgraph(cc))
        for node in cc:
            cc_diams[node] = d

    ref_data_dict = {'spl':comp_pw_shortest_path_lens,
                    'penalty':penalty,
                    'isolates':list(nx.isolates(ref_ngram_net.G)),
                    'CCs':ccs,
                    'cc_diameters':cc_diams,
                    'G':ref_ngram_net.G}

    return ref_data_dict

def convert_2_uniprotIDs(df, id_conv, conv_col = 'Gene stable ID', data_id_cols = 'gene_id', dbl_check = False):
    '''
    This takes a dataframe of interest and converts the specified column to UniProt IDs given a second dataframe consisting of UniProt IDs, ENSEMBL ids, and gene names/synonyms retrieved using biopython. If desired there will be a double check to ensure all IDs are retrieved if an archived version of ENSEMBL has been used and gene names are requested instead.
    '''

    # Now getting all possible UniProt IDs given the IDs of interest.
    ensembl_uni_dict = create_id_conv(df, id_conv=id_conv,conv_col=conv_col, data_id_cols=data_id_cols, dbl_check=dbl_check)
    uniprot_IDs = set(itertools.chain.from_iterable(ensembl_uni_dict.values()))

    return uniprot_IDs

def create_id_conv(df, id_conv, conv_col = 'Gene stable ID', data_id_cols = 'gene_id', dbl_check = False):
    '''
    This is an internal function that makes a dict to convert the ensembl IDs (by default) to UniProt IDs. 
    '''
    # Check if a list is provided for the data_id_cols or if a single/default value provided.
    if isinstance(data_id_cols, list):
        if dbl_check:
            data_id_col = data_id_cols[0]
        else:
            data_id_col = data_id_cols[0]
            warnings.warn('More than one column name was provided for converting, but a double check was not designated. Ignoring additional inputs.')
    else:
        data_id_col = data_id_cols

    # Retrieve all ENSEMBL IDs which are found within the dataset of interest
    success_mapping_ids = set(id_conv[id_conv[conv_col].isin(df[data_id_col])]['Gene stable ID'])
    
    if dbl_check:
        gene_dbl_chck = df[~df[data_id_col].isin(success_mapping_ids)][data_id_cols[1]].dropna()
        cands = id_conv[id_conv['Gene name'].isin(gene_dbl_chck)]
        success_mapping_ids.update(cands['Gene stable ID'])

    id_dict = id_conv[id_conv['Gene stable ID'].isin(success_mapping_ids)].filter(['Gene stable ID', 'UniProtKB/Swiss-Prot ID']).drop_duplicates().dropna().to_dict('tight')
    id_dict = id_dict['data']

    # Now to ensure that all potential UniProt IDs are accounted for have to go through and put them in lists as some Ensembl IDs map to multiple proteins.
    id_conversion = {}
    for conversion_data in id_dict:
        ensembl = conversion_data[0]
        uniprot = conversion_data[1]
        if ensembl in id_conversion:
            id_conversion[ensembl].append(uniprot)
        else:
            id_conversion[ensembl] = [uniprot]

    return id_conversion